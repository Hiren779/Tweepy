{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK 1\n",
    "\n",
    "\n",
    "from tweepy.streaming import StreamListener\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy import Stream\n",
    "import datetime as dt\n",
    "import time\n",
    "import json\n",
    "import pandas as pd\n",
    "try:\n",
    "    import urlparse\n",
    "except ImportError:\n",
    "    import urllib.parse as urlparse\n",
    "import re\n",
    "\n",
    "\n",
    "\n",
    "ACCESS_TOKEN = \"1078653960314122240-5Bk1L531cMZ9dtB39YTWHxdqbrMXcU\"\n",
    "ACCESS_TOKEN_SECRET = \"k9ZS7sxbVINDwgUAB6qCMHmOxPUNbrrxKzAQHgYwRzDUc\"\n",
    "CONSUMER_KEY = \"GSaVJHUfTRVKrD5pzL2kGeKq8\"\n",
    "CONSUMER_SECRET = \"rtaMUnyrpJLBA1zSv9wtXnxTCNwdlN3f1lqmrv3UHpaDQv4dd7\"\n",
    "\n",
    "\n",
    "class TwitterStreamer():\n",
    "    \"\"\"\n",
    "    Class for streaming and processing live tweets.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def stream_tweets(self, fetched_tweets_filename, fetched_links_filename,hash_tag_list,fetched_text_filename):\n",
    "        # This handles Twitter authetification and the connection to Twitter Streaming API\n",
    "        listener = StdOutListener(fetched_tweets_filename)\n",
    "        auth = OAuthHandler(CONSUMER_KEY, CONSUMER_SECRET)\n",
    "        auth.set_access_token(ACCESS_TOKEN, ACCESS_TOKEN_SECRET)\n",
    "        stream = Stream(auth, listener)\n",
    "\n",
    "        # This line filter Twitter Streams to capture data by the keywords: \n",
    "        stream.filter(languages=[\"en\"],track=hash_tag_list)\n",
    "\n",
    "\n",
    "# # # # TWITTER STREAM LISTENER # # # #\n",
    "class StdOutListener(StreamListener):\n",
    "    \"\"\"\n",
    "    This is a basic listener that just prints received tweets to stdout.\n",
    "    \"\"\"\n",
    "    def __init__(self, fetched_tweets_filename,time_limit=15):\n",
    "        self.fetched_tweets_filename = fetched_tweets_filename\n",
    "        self.fetched_links_filename = fetched_links_filename\n",
    "        self.fetched_text_filename = fetched_text_filename\n",
    "        self.start_time = time.time()\n",
    "        self.limit = time_limit\n",
    "        super(StdOutListener, self).__init__()\n",
    "\n",
    "    def on_status(self, status):\n",
    "        if (time.time() - self.start_time) < self.limit:\n",
    "            try:\n",
    "                with open(self.fetched_tweets_filename, 'a') as tf,open(self.fetched_links_filename,'a') as lf,open(self.fetched_text_filename,'a') as wf:\n",
    "                    for url in status.entities['urls']:\n",
    "                        lf.write(str(url['expanded_url']+\" \"))\n",
    "                    \n",
    "                    wf.write(str(status.text))\n",
    "                    tf.write(str(status.user.screen_name+\" \"))\n",
    "                return True\n",
    "            except BaseException as e:\n",
    "                print(\"Error on_status %s\" % str(e))\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "          \n",
    "\n",
    "    def on_error(self, status):\n",
    "        print(status)\n",
    "        \n",
    "        \n",
    "class xyz:\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def no_of_tweets(self):\n",
    "        f = open(\"task1_tweets.txt\", \"r\")\n",
    "        x = f.read()\n",
    "        lst = x.split(\" \")\n",
    "        y = dict((x,lst.count(x)) for x in set(lst))\n",
    "        del y['']\n",
    "        user_name = []\n",
    "        No_of_tweets = []\n",
    "        pd.set_option('display.max_columns', None)\n",
    "        pd.set_option('display.max_rows', None)\n",
    "        for i in y : \n",
    "            user_name.append(i)\n",
    "            No_of_tweets.append(int(y[i]))\n",
    "        df = pd.DataFrame({'user_name':user_name ,\n",
    "                           'No_of_tweets': No_of_tweets})\n",
    "        return df\n",
    "    \n",
    "    def links(self):\n",
    "        f = open(\"task2_links.txt\", \"r\")\n",
    "        x = f.read()\n",
    "        lst = x.split(\" \")\n",
    "        link_list = []\n",
    "\n",
    "        for string in lst:\n",
    "            parsed = urlparse.urlparse(string)\n",
    "            domain = parsed.netloc.split(\".\")[0:]\n",
    "            host = \".\".join(domain)\n",
    "            link_list.append(host)\n",
    "        link_list.remove(\"\")\n",
    "\n",
    "        y = dict((x,link_list.count(x)) for x in set(link_list))\n",
    "        pd.set_option('display.max_columns', None)\n",
    "        pd.set_option('display.max_rows', None)\n",
    "        domain , no_of_domain= [],[]\n",
    "\n",
    "        for i in y : \n",
    "            domain.append(i)\n",
    "            no_of_domain.append(int(y[i]))\n",
    "        df = pd.DataFrame({'domain':domain ,\n",
    "                           'number_of_occurence': no_of_domain})\n",
    "        df = df.sort_values(by='number_of_occurence', ascending=False)\n",
    "        print(\"Total number of links is :\",len(link_list))\n",
    "        return df    \n",
    "\n",
    "    def words(self):\n",
    "        common_words = ['can','her','was','will','dont','your','if','its','from','like','are','but','we','and','you','for','this','with','me','not','be','is','so','im','by','to','on','of', 'at', 'in', 'without', 'between', 'he', 'they', 'anybody', 'it', 'one','the', 'a', 'that', 'my', 'more', 'much', 'either', 'neither','i','have', 'got', 'do','no','that', 'when','']\n",
    "        f = open(\"task1_words.txt\", \"r\")\n",
    "        x = f.read()\n",
    "        x = \" \".join(re.sub(\"([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \"\", x).split()) # here we are removing emojis and extra garbage which isnt word\n",
    "        words_list = x.split(\" \")\n",
    "        lower_case = [word.lower() for word in words_list]\n",
    "\n",
    "        new_words = [word for word in lower_case if word not in common_words]\n",
    "        y = dict((x,new_words.count(x)) for x in set(new_words))\n",
    "        word = []\n",
    "        count_of_words = []\n",
    "        pd.set_option('display.max_columns', None)\n",
    "        pd.set_option('display.max_rows', None)\n",
    "        for i in y : \n",
    "            word.append(i)\n",
    "            count_of_words.append(int(y[i]))\n",
    "        df = pd.DataFrame({'word':word ,\n",
    "                           'count_of_words': count_of_words})\n",
    "        df = df.sort_values(by='count_of_words', ascending=False)\n",
    "        df = df.head(10)\n",
    "        return df\n",
    "    \n",
    "    def erase(self):\n",
    "        open(\"task1_tweets.txt\", \"w\").close()\n",
    "        open(\"task1_links.txt\", \"w\").close()\n",
    "        open(\"task1_words.txt\", \"w\").close()\n",
    "        \n",
    "\n",
    " \n",
    "if __name__ == '__main__':\n",
    " \n",
    "    # Authenticate using config.py and connect to Twitter Streaming API.\n",
    "    \n",
    "    keyword = input(\" please Enter keyword you want to search: \")\n",
    "    hash_tag_list = [keyword]\n",
    "    fetched_tweets_filename = \"task1_tweets.txt\"\n",
    "    fetched_links_filename = \"task1_links.txt\"\n",
    "    fetched_text_filename = \"task1_words.txt\"\n",
    "\n",
    "    \n",
    "    while True:\n",
    "        twitter_streamer = TwitterStreamer()\n",
    "        obj1 = xyz()\n",
    "        twitter_streamer.stream_tweets(fetched_tweets_filename, hash_tag_list,fetched_links_filename,fetched_text_filename)\n",
    "        print(\"User report \\n\",obj1.no_of_tweets())\n",
    "        print(\"Links report\\n\",obj1.links())\n",
    "        print(\"Word report\\n\",obj1.words())\n",
    "        obj1.erase()  # delete content after one minute\n",
    "         \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
